{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1a8ea2",
   "metadata": {},
   "source": [
    "# 1) OpenPose를 이용해서 이미지를 구현하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26f46b",
   "metadata": {},
   "source": [
    "#OpenPose는 OpenCv2를 이용해서 불러올수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f17cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00bd94",
   "metadata": {},
   "source": [
    "# 1.포인트찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489851b",
   "metadata": {},
   "source": [
    "#OpenPose를 이용할때는 OpenPose파일 네트워크를 따로 로드해야 이용이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e7aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_keypoints(frame, proto_file, weights_file, threshold, model_name, BODY_PARTS):\n",
    "    global points\n",
    "    # 네트워크 불러오기\n",
    "    net = cv2.dnn.readNetFromCaffe(proto_file, weights_file)\n",
    "\n",
    "    # 입력 이미지의 사이즈 정의\n",
    "    image_height = 368\n",
    "    image_width = 368\n",
    "\n",
    "    # 네트워크에 넣기 위한 전처리\n",
    "    input_blob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (image_width, image_height), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    # mean: 만약 image의 채널이 BGR 순서이고, swapRB가 true이면 R 평균, G평균, B 평균 순서로 값 지정\n",
    "    # swapRB : 첫 번째 채널과 3번째 채널을 서로 바꿀 것인지를 결정하는 flag.\n",
    "    # 이 값이 true이면 컬러 입력 영상의 채널 순서를 BGR에서 RGB로 변경\n",
    "    # crop : 입력 영상(image)의 크기를 변경한 후, crop을 수행할 것인지를 결정하는 flag\n",
    "    # network입력을 위한 blob으로 변환(blob: 영상 등의 data를 포함할수 있는 다차원 data 표현 방식)\n",
    "    \n",
    "    # 전처리된 blob 네트워크에 입력\n",
    "    net.setInput(input_blob)\n",
    "\n",
    "    # 결과 받아오기\n",
    "    out = net.forward()\n",
    "    out_height = out.shape[2]\n",
    "    out_width = out.shape[3]\n",
    "\n",
    "    # 원본 이미지의 높이, 너비를 받아오기\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    \n",
    "    \n",
    "     # 포인트 리스트 초기화\n",
    "    points = []\n",
    "    print(f\"\\n============================== {model_name} Model ==============================\")\n",
    "\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "\n",
    "        # 신체 부위의 confidence map\n",
    "        prob_map = out[0, i, :, :]\n",
    "\n",
    "        # 최소값, 최대값, 최소값 위치, 최대값 위치\n",
    "        min_val, prob, min_loc, point = cv2.minMaxLoc(prob_map)\n",
    "\n",
    "        # 원본 이미지에 맞게 포인트 위치 조정\n",
    "        x = (frame_width * point[0]) / out_width\n",
    "        x = int(x)\n",
    "        y = (frame_height * point[1]) / out_height\n",
    "        y = int(y)\n",
    "\n",
    "        if prob > threshold:  # [pointed] \n",
    "            # cv.circle(그림, 중심, 크기, 색상, 두께(-1이면 꽉채움), 선 유형)\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "            # cv2.putText(그림, 문구, 시작좌표, 폰트종류, 폰트크기, 색상, 사이즈, 선 유형)\n",
    "            cv2.putText(frame, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            points.append((x, y))\n",
    "            print(f\"[pointed] {BODY_PARTS[i]} ({i}) => prob: {prob:.5f} / x: {x} / y: {y}\")\n",
    "\n",
    "        else:  # [not pointed]\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "            cv2.putText(frame, str(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            points.append(None)\n",
    "            print(f\"[not pointed] {BODY_PARTS[i]} ({i}) => prob: {prob:.5f} / x: {x} / y: {y}\")\n",
    "\n",
    "    cv2.imshow(\"Output_Keypoints\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    return frame\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d30f0",
   "metadata": {},
   "source": [
    " # 2. 각 포인트의 좌표끼리 선 그리기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f824db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def output_keypoints_with_lines(frame, POSE_PAIRS):\n",
    "    print()\n",
    "    for pair in POSE_PAIRS:\n",
    "        part_a = pair[0]  # 0 (Head)\n",
    "        part_b = pair[1]  # 1 (Neck)\n",
    "        if points[part_a] and points[part_b]:  # 선 그리기\n",
    "            print(f\"[linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
    "            cv2.line(frame, points[part_a], points[part_b], (0, 255, 0), 3)\n",
    "        else:  # 선 그리지 않기\n",
    "            print(f\"[not linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
    "\n",
    "    cv2.imshow(\"output_keypoints_with_lines\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f785d",
   "metadata": {},
   "source": [
    "# 3. 포인트 선 이미지화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5cf9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포인트 선 이미지화\n",
    "def output_lines_image(frame, POSE_PAIRS):\n",
    "    print()\n",
    "    for pair in POSE_PAIRS:\n",
    "        part_a = pair[0]  # 0 (Head)\n",
    "        part_b = pair[1]  # 1 (Neck)\n",
    "        if points[part_a] and points[part_b]:  # 선 그리기\n",
    "            print(f\"[linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
    "            cv2.line(frame, points[part_a], points[part_b], (0, 255, 0), 3)\n",
    "        else:  # 선 그리지 않기\n",
    "            print(f\"[not linked] {part_a} {points[part_a]} <=> {part_b} {points[part_b]}\")\n",
    "\n",
    "    cv2.imshow(\"output_keypoints_with_lines\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4467b0",
   "metadata": {},
   "source": [
    "#OpenPose의 모델이 여러개가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4504f9e",
   "metadata": {},
   "source": [
    "#MPII(16개),COCO(19개), BODY_25(26개) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca8b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== BODY_25 Model ==============================\n",
      "[pointed] Nose (0) => prob: 0.87486 / x: 101 / y: 31\n",
      "[pointed] Neck (1) => prob: 0.61511 / x: 101 / y: 74\n",
      "[pointed] RShoulder (2) => prob: 0.50242 / x: 66 / y: 84\n",
      "[pointed] RElbow (3) => prob: 0.73921 / x: 61 / y: 159\n",
      "[pointed] RWrist (4) => prob: 0.73808 / x: 55 / y: 233\n",
      "[pointed] LShoulder (5) => prob: 0.41534 / x: 137 / y: 74\n",
      "[pointed] LElbow (6) => prob: 0.65056 / x: 152 / y: 159\n",
      "[pointed] LWrist (7) => prob: 0.69867 / x: 162 / y: 222\n",
      "[pointed] MidHip (8) => prob: 0.42615 / x: 106 / y: 180\n",
      "[pointed] RHip (9) => prob: 0.37304 / x: 86 / y: 180\n",
      "[pointed] RKnee (10) => prob: 0.60111 / x: 86 / y: 318\n",
      "[pointed] RAnkle (11) => prob: 0.59113 / x: 91 / y: 434\n",
      "[pointed] LHip (12) => prob: 0.37615 / x: 127 / y: 180\n",
      "[pointed] LKnee (13) => prob: 0.66082 / x: 137 / y: 307\n",
      "[pointed] LAnkle (14) => prob: 0.55917 / x: 142 / y: 434\n",
      "[pointed] REye (15) => prob: 0.89886 / x: 96 / y: 21\n",
      "[pointed] LEye (16) => prob: 0.84974 / x: 111 / y: 21\n",
      "[pointed] REar (17) => prob: 0.64987 / x: 81 / y: 21\n",
      "[pointed] LEar (18) => prob: 0.77122 / x: 122 / y: 21\n",
      "[pointed] LBigToe (19) => prob: 0.45997 / x: 162 / y: 456\n",
      "[pointed] LSmallToe (20) => prob: 0.54061 / x: 162 / y: 456\n",
      "[pointed] LHeel (21) => prob: 0.34253 / x: 142 / y: 445\n",
      "[pointed] RBigToe (22) => prob: 0.46780 / x: 76 / y: 445\n",
      "[pointed] RSmallToe (23) => prob: 0.45895 / x: 76 / y: 445\n",
      "[pointed] RHeel (24) => prob: 0.48191 / x: 101 / y: 445\n",
      "[pointed] Background (25) => prob: 1.01174 / x: 96 / y: 286\n",
      "\n",
      "[linked] 0 (101, 31) <=> 1 (101, 74)\n",
      "[linked] 0 (101, 31) <=> 15 (96, 21)\n",
      "[linked] 0 (101, 31) <=> 16 (111, 21)\n",
      "[linked] 1 (101, 74) <=> 2 (66, 84)\n",
      "[linked] 1 (101, 74) <=> 5 (137, 74)\n",
      "[linked] 1 (101, 74) <=> 8 (106, 180)\n",
      "[linked] 8 (106, 180) <=> 9 (86, 180)\n",
      "[linked] 8 (106, 180) <=> 12 (127, 180)\n",
      "[linked] 9 (86, 180) <=> 10 (86, 318)\n",
      "[linked] 12 (127, 180) <=> 13 (137, 307)\n",
      "[linked] 2 (66, 84) <=> 3 (61, 159)\n",
      "[linked] 3 (61, 159) <=> 4 (55, 233)\n",
      "[linked] 5 (137, 74) <=> 6 (152, 159)\n",
      "[linked] 6 (152, 159) <=> 7 (162, 222)\n",
      "[linked] 10 (86, 318) <=> 11 (91, 434)\n",
      "[linked] 13 (137, 307) <=> 14 (142, 434)\n",
      "[linked] 15 (96, 21) <=> 17 (81, 21)\n",
      "[linked] 16 (111, 21) <=> 18 (122, 21)\n",
      "[linked] 14 (142, 434) <=> 21 (142, 445)\n",
      "[linked] 19 (162, 456) <=> 21 (142, 445)\n",
      "[linked] 20 (162, 456) <=> 21 (142, 445)\n",
      "[linked] 11 (91, 434) <=> 24 (101, 445)\n",
      "[linked] 22 (76, 445) <=> 24 (101, 445)\n",
      "[linked] 23 (76, 445) <=> 24 (101, 445)\n",
      "\n",
      "[linked] 0 (101, 31) <=> 1 (101, 74)\n",
      "[linked] 0 (101, 31) <=> 15 (96, 21)\n",
      "[linked] 0 (101, 31) <=> 16 (111, 21)\n",
      "[linked] 1 (101, 74) <=> 2 (66, 84)\n",
      "[linked] 1 (101, 74) <=> 5 (137, 74)\n",
      "[linked] 1 (101, 74) <=> 8 (106, 180)\n",
      "[linked] 8 (106, 180) <=> 9 (86, 180)\n",
      "[linked] 8 (106, 180) <=> 12 (127, 180)\n",
      "[linked] 9 (86, 180) <=> 10 (86, 318)\n",
      "[linked] 12 (127, 180) <=> 13 (137, 307)\n",
      "[linked] 2 (66, 84) <=> 3 (61, 159)\n",
      "[linked] 3 (61, 159) <=> 4 (55, 233)\n",
      "[linked] 5 (137, 74) <=> 6 (152, 159)\n",
      "[linked] 6 (152, 159) <=> 7 (162, 222)\n",
      "[linked] 10 (86, 318) <=> 11 (91, 434)\n",
      "[linked] 13 (137, 307) <=> 14 (142, 434)\n",
      "[linked] 15 (96, 21) <=> 17 (81, 21)\n",
      "[linked] 16 (111, 21) <=> 18 (122, 21)\n",
      "[linked] 14 (142, 434) <=> 21 (142, 445)\n",
      "[linked] 19 (162, 456) <=> 21 (142, 445)\n",
      "[linked] 20 (162, 456) <=> 21 (142, 445)\n",
      "[linked] 11 (91, 434) <=> 24 (101, 445)\n",
      "[linked] 22 (76, 445) <=> 24 (101, 445)\n",
      "[linked] 23 (76, 445) <=> 24 (101, 445)\n"
     ]
    }
   ],
   "source": [
    "BODY_PARTS_BODY_25 = {0: \"Nose\", 1: \"Neck\", 2: \"RShoulder\", 3: \"RElbow\", 4: \"RWrist\",\n",
    "                      5: \"LShoulder\", 6: \"LElbow\", 7: \"LWrist\", 8: \"MidHip\", 9: \"RHip\",\n",
    "                      10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\", 14: \"LAnkle\",\n",
    "                      15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\",\n",
    "                      20: \"LSmallToe\", 21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\", 25: \"Background\"}\n",
    "\n",
    "POSE_PAIRS_BODY_25 = [[0, 1], [0, 15], [0, 16], [1, 2], [1, 5], [1, 8], [8, 9], [8, 12], [9, 10], [12, 13], [2, 3],\n",
    "                      [3, 4], [5, 6], [6, 7], [10, 11], [13, 14], [15, 17], [16, 18], [14, 21], [19, 21], [20, 21],\n",
    "                      [11, 24], [22, 24], [23, 24]]\n",
    "\n",
    "# 신경 네트워크의 구조를 지정하는 prototxt 파일 (다양한 계층이 배열되는 방법 등)\n",
    "protoFile_body_25 = \"C:\\\\Users\\\\flors\\\\position\\\\pose_deploy.prototxt\"\n",
    "\n",
    "# 훈련된 모델의 weight 를 저장하는 caffemodel 파6\n",
    "weightsFile_body_25 = \"C:\\\\Users\\\\flors\\\\position\\\\pose_iter_584000.caffemodel\"\n",
    "\n",
    "# 이미지 경로\n",
    "man = 'C:\\\\Users\\\\flors\\\\Documents\\\\fotos\\\\25.png'\n",
    "man2 = \"C:\\\\Users\\\\flors\\\\position\\\\H.png\"\n",
    "# 키포인트를 저장할 빈 리스트\n",
    "points = []\n",
    "\n",
    "# 이미지 읽어오기\n",
    "frame_body_25 = cv2.imread(man)\n",
    "frame_body = cv2.imread(man2)\n",
    "\n",
    "\n",
    "\n",
    "# BODY_25 Model\n",
    "frame_BODY_25 = output_keypoints(frame=frame_body_25, proto_file=protoFile_body_25, weights_file=weightsFile_body_25,\n",
    "                             threshold=0.2, model_name=\"BODY_25\", BODY_PARTS=BODY_PARTS_BODY_25)\n",
    "output_keypoints_with_lines(frame=frame_body_25, POSE_PAIRS=POSE_PAIRS_BODY_25)\n",
    "output_lines_image(frame=frame_body, POSE_PAIRS=POSE_PAIRS_BODY_25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
